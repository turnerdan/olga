# OLGA: Ontology-Linked Generalized Assistant

**OLGA** is a modular, local-first AI agent framework combining LangGraph state routing with Retrieval-Augmented Generation (RAG) and semantic memory. Designed for professionals, analysts, and builders, OLGA supports offline operation, persona-aware responses, and extensible task orchestration â€” all from your terminal.

---

## ğŸ§  Intelligent Local Agents, Zero Cloud Dependency

OLGA orchestrates intelligent, persona-aware logic using structured flows:

- ğŸ§¾ **Embedded Memory Recall** â€” vectorized markdown memory for grounded responses  
- ğŸ•¸ **Web Fallback** â€” live web scraping + summarization (via DuckDuckGo)  
- ğŸ§­ **Dynamic Routing** â€” classifies user input and selects the best tool  
- ğŸª¶ **Mode-Aware Output** â€” output adapts to `work`, `play`, or `shared` memory state  
- ğŸ”’ **Privacy-First** â€” 100% offline-capable, no cloud calls required  

---

## ğŸ“ Current Directory Layout

```
LangChain_Olga/
â”œâ”€â”€ app.py                  # CLI runner and LangGraph execution
â”œâ”€â”€ chat.py                 # Chat memory handler
â”œâ”€â”€ rag.py                  # RAG functions and FAISS logic
â”œâ”€â”€ langgraph_router.py     # LangGraph state logic and graph compilation
â”œâ”€â”€ routing_classifier.py   # Query router (chat vs rag vs web)
â”œâ”€â”€ ui_formatter.py         # Output formatting and CLI presentation
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ responding.py       # Tool wrappers for chat, rag, web
â”‚   â””â”€â”€ web.py              # DuckDuckGo HTML search tool
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ model_router.py     # Model routing + embedding model loader
â”‚
â”œâ”€â”€ personalities/
â”‚   â”œâ”€â”€ work.md             # Embedded persona memory (work mode)
â”‚   â”œâ”€â”€ play.md             # Embedded persona memory (play mode)
â”‚   â””â”€â”€ shared/             # (planned) universal facts & context
â”‚
â”œâ”€â”€ faiss_indexes/
â”‚   â””â”€â”€ work/index.faiss    # FAISS index for memory embeddings
â”‚
â””â”€â”€ README.md               # You are here
```

---

## âš™ï¸ Core Functions

| Function | Description |
|----------|-------------|
| `run_graph()` | Compiles the LangGraph state machine for response execution |
| `search_or_respond_tool()` | Routes input through memory, retrieval, or search |
| `query_persona_store()` | Vector search via FAISS for semantic memory |
| `respond_with_chat_memory()` | Basic conversational fallback (mode-specific) |
| `format_response()` / `pretty_print()` | Output decoration & CLI beautification |
| `setup_logging()` | Logging toggle (quiet by default, verbose with `--debug`) |
| `validate_environment()` | Ensures FAISS index + embedding model are present |

---

## ğŸš€ Usage

```bash
./run.sh              # Launch OLGA in quiet mode with styled output
./run.sh --debug      # Enable verbose logging and CLI tracebacks
./run.sh --bench 5    # Run 5 benchmark prompts using RAG pipeline
```

---

## ğŸ§ª In Development (Planned Features)

### 1. `vectorize_and_embed_documents()`
- Ingests `.docx`, `.pdf`, `.md`
- Converts to vector format for private AI memory querying
- Will support assisted summarization + live Q&A

### 2. `pretty_notify()`
- Live system feedback (terminal banners, desktop alerts, optional voice)
- Use cases: job completions, scheduled events, document ingestion status

### 3. `analyze_and_visualize_data()`
- Visual summaries from raw data (CSV, JSON, transcripts)
- CLI plots + Markdown reports
- Useful for: KPI reporting, assistant diagnostics, memory visualizations

---

## ğŸ”– Version

**0.6-pre** â€” Routing, memory, and LangGraph functional. Assistant extensions underway.
